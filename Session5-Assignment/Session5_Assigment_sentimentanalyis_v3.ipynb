{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session5-Assigment-sentimentanalyis_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOpMXz1fxsmIBJc4uK/mkj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Smruthi3/END2/blob/main/Session5-Assignment/Session5_Assigment_sentimentanalyis_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAT1uIXimolE"
      },
      "source": [
        "## Importing the standford data files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEXy5osOm36W",
        "outputId": "c0c8dee7-d07c-4521-9003-99fa8a7cf8cc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phimi8sJsDJC"
      },
      "source": [
        "### Data overview\n",
        "Based on Readme file given by StandordSentiment Treebank, the following way sentence and sentiment label are mapped\n",
        "\n",
        "1. datasetSentences.txt contains sentence and corresponding index\n",
        "2. datasetSplit contains the sentence index and splitset label. Splitset labels 1,2,3 corresponds to tarin, test and dev respectively\n",
        "3. datasetSentences and  datasetSplit merged based on sentence index. let's call it as merged_dataset\n",
        "4. sentiment_labels contains phrase id and sentiment values ranging from 0 to 1. Here notice that there is no one to mapping between datasetSentences and sentiment_labels\n",
        "5. dictionary.txt contains all phrases and their IDs\n",
        "6. Now, perfom  left join on merged_dataset and dictionary using sentence and phrases\n",
        "7. As we have phrase id, again perform left join on previous dataset (obtained from step #6) and sentiment_labels using phrase_id and phrase id\n",
        "8. Here we get a final table with sentiment values\n",
        "9. Divide the sentiment values into 5 buckets [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0] and assign a label from 0 to 4\n",
        "10. Finally, use a split label to devide your data into train,test and dev\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VAtZkPxnB19"
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "DATA_DIR = 'drive/My Drive/END2/Session5-Assignment/stanfordSentimentTreebank/stanfordSentimentTreebank'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "Ryw7CwEmndxs",
        "outputId": "baa74443-8aae-4ae2-a1b2-85918b8e5182"
      },
      "source": [
        "datasetSentences = pd.read_csv(os.path.join(DATA_DIR, 'datasetSentences.txt'), sep='\\t')\n",
        "datasetSentences.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index                                           sentence\n",
              "0               1  The Rock is destined to be the 21st Century 's...\n",
              "1               2  The gorgeously elaborate continuation of `` Th...\n",
              "2               3                     Effective but too-tepid biopic\n",
              "3               4  If you sometimes like to go to the movies to h...\n",
              "4               5  Emerges as something rare , an issue movie tha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "2EWazAweoouF",
        "outputId": "d5d0e1c4-59cc-4e06-fa28-6834d0f1d80b"
      },
      "source": [
        "datasetSplit = pd.read_csv(os.path.join(DATA_DIR, 'datasetSplit.txt'), sep=',')\n",
        "datasetSplit.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  splitset_label\n",
              "0               1               1\n",
              "1               2               1\n",
              "2               3               2\n",
              "3               4               2\n",
              "4               5               2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dvqj9LEpXuC",
        "outputId": "949a14fe-22ff-49e1-8ba7-36966375947d"
      },
      "source": [
        "datasetSentences.shape, datasetSplit.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11855, 2), (11855, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ra0JZFApqCK",
        "outputId": "5fa2d5a1-7578-4a29-e2ad-89539e8f51c7"
      },
      "source": [
        "datasetSentences.columns, datasetSplit.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['sentence_index', 'sentence'], dtype='object'),\n",
              " Index(['sentence_index', 'splitset_label'], dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "gqdgfaQLp4pK",
        "outputId": "bb8ee1f6-4d3a-44b3-8267-fa1cf51b17ca"
      },
      "source": [
        "merged_dataset = datasetSentences.merge(datasetSplit, on='sentence_index')\n",
        "merged_dataset.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ... splitset_label\n",
              "0               1  ...              1\n",
              "1               2  ...              1\n",
              "2               3  ...              2\n",
              "3               4  ...              2\n",
              "4               5  ...              2\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZLTAEuzp_ba",
        "outputId": "924cb179-7090-46cb-faea-b9aa1943164f"
      },
      "source": [
        "sentiment_label = pd.read_csv(os.path.join(DATA_DIR, \"sentiment_labels.txt\"),sep='|')\n",
        "sentiment_label.head(),sentiment_label.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   phrase ids  sentiment values\n",
              " 0           0           0.50000\n",
              " 1           1           0.50000\n",
              " 2           2           0.44444\n",
              " 3           3           0.50000\n",
              " 4           4           0.42708, (239232, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjGlmoJNqJ2F",
        "outputId": "69d393f2-fc78-4e04-ff60-b3838e0cbf2d"
      },
      "source": [
        "dictionary = pd.read_csv(os.path.join(DATA_DIR, \"dictionary.txt\"), sep='|',header=None)\n",
        "dictionary.columns = ['phrases', 'phrase_id']\n",
        "dictionary.head(),dictionary.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(       phrases  phrase_id\n",
              " 0            !          0\n",
              " 1          ! '      22935\n",
              " 2         ! ''      18235\n",
              " 3       ! Alas     179257\n",
              " 4  ! Brilliant      22936, (239232, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpZ66-6xqTmB",
        "outputId": "ca16e3ad-aa50-4ff7-cdac-0bf8d2622279"
      },
      "source": [
        "dataset_with_phrase_id = merged_dataset.merge(dictionary, left_on='sentence', right_on ='phrases', how='left')\n",
        "dataset_with_phrase_id.head(), dataset_with_phrase_id.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   sentence_index  ... phrase_id\n",
              " 0               1  ...  226166.0\n",
              " 1               2  ...  226300.0\n",
              " 2               3  ...   13995.0\n",
              " 3               4  ...   14123.0\n",
              " 4               5  ...   13999.0\n",
              " \n",
              " [5 rows x 5 columns], (11855, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORibhsIkrOtd"
      },
      "source": [
        "ads = dataset_with_phrase_id.merge(sentiment_label, left_on='phrase_id', right_on='phrase ids', how='left')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c5247BqrXyb"
      },
      "source": [
        "ads['labels'] = [(0 if 0 <=i <=0.2 else (1 if 0.2<i<=0.4 else (2 if 0.4 <i<=0.6 else (3 if 0.6 <i<=0.8 else 4)))) for i in ads['sentiment values'] ]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "MyCarf0Mwp2v",
        "outputId": "f64e5b51-86cc-4c81-ea5a-5a2d891f7798"
      },
      "source": [
        "ads.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>phrases</th>\n",
              "      <th>phrase_id</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>1</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>0.83333</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>2</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>0.51389</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>2</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>0.73611</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>2</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>0.86111</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ... labels\n",
              "0               1  ...      3\n",
              "1               2  ...      4\n",
              "2               3  ...      2\n",
              "3               4  ...      3\n",
              "4               5  ...      4\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VVCV1x9w7dt"
      },
      "source": [
        "train_df, test_df, dev_df = [groups for name, groups in ads.groupby('splitset_label')]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tysavTKJxF8X",
        "outputId": "e1e01514-c5b2-4cde-904e-b0cc764cd410"
      },
      "source": [
        "ads.shape[0] == train_df.shape[0] + test_df.shape[0] + dev_df.shape[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSBvU55TIv8O"
      },
      "source": [
        "train_df=train_df[['sentence','labels']]\n",
        "train_df=train_df.reset_index(drop=True)\n",
        "test_df=test_df[['sentence','labels']]\n",
        "test_df=test_df.reset_index(drop=True)\n",
        "dev_df=dev_df[['sentence','labels']]\n",
        "dev_df=dev_df.reset_index(drop=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwhNinbOIv-w",
        "outputId": "d776bc5d-9f73-493d-cbe1-94a3324ba2a5"
      },
      "source": [
        "train_df.shape,test_df.shape,dev_df.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8544, 2), (2210, 2), (1101, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Y6N6ljhmXC",
        "outputId": "4f0ea7bf-1718-4c40-e0b3-71112d80a060"
      },
      "source": [
        "train_df.labels.value_counts()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2216\n",
              "1    2092\n",
              "4    1657\n",
              "2    1549\n",
              "0    1030\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oJnrUiUJGUS"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTA50QFC8_sy"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Intially tried for all the sentences in the training data set. It look lot of time but did not convert more than few sentences so for the demonstartion purpose the below mentioned approuch is used\n",
        "\n",
        "Due to following limitations of the package only a random smaple of 10 sentences (why 10 just randonly chosen) are taken and performed this augmentation. Germen language is chosen here (one can choose multiple langauges too)\n",
        "  1. The maximum character limit on a single text is 15k.\n",
        "  2. Due to limitations of the web version of google translate, this API does not guarantee that the library would work properly at all times\n",
        "\n",
        "If there is time, probably back transaltion can be done on entire training set by passing sentences in multiple batches on different days/time \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWC2tLVOJ-oJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71573ab8-ed6d-4e4f-a22a-2160c82a25b9"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 14.4MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.5MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=5c299c10d4c8c253b1a007344f460e28a3575d75cb7ab0b2099d650f69400488\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, sniffio, hstspreload, hyperframe, hpack, h2, h11, httpcore, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbW-TnVk6q-b"
      },
      "source": [
        "sample = train_df.sample(10)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv6_HPawWAk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542f1173-e74a-4a02-d385-049e61597a45"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        " \n",
        "translator = googletrans.Translator()\n",
        "sentence = list(sample.sentence)\n",
        "print(len(sentence))\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = 'de' #random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "#print(translations)\n",
        "t_text = [t.text for t in translations]\n",
        "print(t_text)\n",
        "\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "print(en_text)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "Translating to german\n",
            "['Es ist wahrscheinlich, dass Sie, was immer Sie von der ersten Produktion dachten – für oder gegen –, wahrscheinlich an diese denken werden.', 'Ein gelungener Film von selbstreflexiver, philosophischer Natur.', 'Ver Wiels verzweifelter Versuch des Witzes geht verloren und der Charakter von Critical Jim wird zweidimensional und sinnlos.', 'Erwachsene Spitzfindigkeiten gehören hier nicht zur Sache.', 'Queen of the Damned zu sehen ist wie das Lesen einer Forschungsarbeit mit Spezialeffekten.', 'Eine lächerliche – oder eher unlächerliche – Ausrede für einen Film.', 'Kritiker brauchen auch ein gutes Lachen, und diese für das Fernsehen zu extreme Wiedergabe der berüchtigten MTV-Show liefert die unverschämten, widerlichen, abstoßenden Waren in dampfenden, viszeralen Haufen.', 'Der Anblick des Raumschiffs auf der Startrampe ist in IMAX-Dimensionen gebührend beeindruckend, ebenso wie Aufnahmen der in ihren Kabinen schwebenden Astronauten.', 'Der Film hat die unheimliche Fähigkeit, sich genau dann zu korrigieren, wenn man denkt, dass er in Gefahr ist, schief zu gehen.', 'Der Komediant ist eine Geschichte, die es wert ist, aufgenommen zu werden.']\n",
            "['Chances are that whatever you thought - for or against - about the first production, you probably will think of this one.', 'A successful film of a self-reflective, philosophical nature.', \"Ver Wiel's desperate attempt to joke is lost, and Critical Jim's character becomes two-dimensional and pointless.\", 'Adult quibbles are out of the question here.', 'Watching Queen of the Damned is like reading a special effects research paper.', 'A ridiculous - or rather ridiculous - excuse for a movie.', 'Critics need a good laugh too, and this too extreme for television rendition of the infamous MTV show delivers the outrageous, disgusting, repulsive goods in steamy, visceral piles.', 'The sight of the spaceship on the launch pad is appropriately impressive in IMAX dimensions, as are images of the astronauts floating in their cabins.', 'The film has an uncanny ability to correct itself right when you think it is in danger of going wrong.', 'The comedian is a story worth including.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akb-MG6kYWtJ",
        "outputId": "becc6d4a-7045-4809-d042-1afc75ad714c"
      },
      "source": [
        "d = {'sentence':en_text,'labels':list(sample.labels)}\n",
        "d1=pd.DataFrame(d)\n",
        "d1.shape\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZmLCklbc47p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06b31b1-bc75-4a7e-e764-3ffbf72d9f42"
      },
      "source": [
        "train_df_agumented = pd.concat([train_df,d1])\n",
        "train_df_agumented = train_df_agumented.reset_index(drop=True)\n",
        "train_df_agumented.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8554, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ7mehfu9NW4"
      },
      "source": [
        "### Radom swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19pwfeviZI18"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEzYYp3x9YWh"
      },
      "source": [
        "data_rand_swap=pd.DataFrame()\n",
        "data_rand_swap['sentence']=train_df['sentence'].apply(lambda x : \" \".join(random_swap(x.split(' '))))\n",
        "data_rand_swap['labels'] = train_df['labels'] "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiOxMPMhBPJi",
        "outputId": "e36bac72-a49a-4ac7-de58-d871263ceaa7"
      },
      "source": [
        "train_df_agumented1=pd.concat([train_df_agumented,data_rand_swap])\n",
        "train_df_agumented1=train_df_agumented1.reset_index(drop=True)\n",
        "train_df_agumented1.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17098, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bc4cU_ABnDU"
      },
      "source": [
        "### Random Deletion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kw1zkbYeLAD"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-IyaIrrZoh2"
      },
      "source": [
        "data_rand_del=pd.DataFrame()\n",
        "data_rand_del['sentence'] = train_df['sentence'].apply(lambda x : \" \".join(random_deletion(x.split(' '))))\n",
        "data_rand_del['labels'] = train_df['labels'] "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEUu5fSTCLKK",
        "outputId": "1abc5994-5891-4091-a708-1d5599dd1780"
      },
      "source": [
        "train_df_agumented2=pd.concat([train_df_agumented1,data_rand_del])\n",
        "train_df_agumented2=train_df_agumented2.reset_index(drop=True)\n",
        "train_df_agumented2.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25642, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYlUuzyHrPy"
      },
      "source": [
        "#train_df_agumented2.to_csv(\"drive/My Drive/END2/Session5-Assignment/train_df_agumented2.csv\",index=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh89YImhEpfX"
      },
      "source": [
        "### Defining Data Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o56V5L6PCcEv",
        "outputId": "debcf838-1d39-4ee3-b5ed-9dfb148b6275"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa3b33943d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2HpASKSE1NM"
      },
      "source": [
        "Review = data.Field(sequential = True , tokenize ='spacy',batch_first = True , include_lengths =True)\n",
        "Label = data.LabelField(tokenize ='spacy',is_target=True,batch_first=True,sequential=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIKcC6qeFZfp"
      },
      "source": [
        "fields = [('reviews', Review),('labels',Label)]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVKd0DwBLr1V",
        "outputId": "e6a059f7-2fc0-45cb-ee15-ef4e3aabe975"
      },
      "source": [
        "train_df_agumented2.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25642, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5RBHBZxMO2J"
      },
      "source": [
        "train_example = [data.Example.fromlist([train_df_agumented2.sentence[i],train_df_agumented2.labels[i]], fields) for i in range(train_df_agumented2.shape[0])] \n",
        "test_example = [data.Example.fromlist([test_df.sentence[i],test_df.labels[i]], fields) for i in range(test_df.shape[0])] \n",
        "valid_example = [data.Example.fromlist([dev_df.sentence[i],dev_df.labels[i]],fields) for i in range(dev_df.shape[0])]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNd5rOkTLJlv"
      },
      "source": [
        "train = data.Dataset(train_example, fields)\n",
        "valid = data.Dataset(valid_example, fields)\n",
        "test = data.Dataset(test_example, fields)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHYqtEq7SrhN",
        "outputId": "8b141c0f-7f8b-45b7-9e6f-18b543df27d5"
      },
      "source": [
        "(len(train), len(test),len(valid))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25642, 2210, 1101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6yAtlMTSwUe",
        "outputId": "fe1c9e8f-1400-4490-b1e7-be50f76ff8c2"
      },
      "source": [
        "vars(train.examples[10])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 4,\n",
              " 'reviews': ['Good',\n",
              "  'fun',\n",
              "  ',',\n",
              "  'good',\n",
              "  'action',\n",
              "  ',',\n",
              "  'good',\n",
              "  'acting',\n",
              "  ',',\n",
              "  'good',\n",
              "  'dialogue',\n",
              "  ',',\n",
              "  'good',\n",
              "  'pace',\n",
              "  ',',\n",
              "  'good',\n",
              "  'cinematography',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2eBgtbwwNGm"
      },
      "source": [
        "#### Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAu3rEArSygI",
        "outputId": "91dd61c7-1cd8-400e-cbb8-748a67cd59e4"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "Review.build_vocab(train, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                           \n",
            " 99%|█████████▉| 397880/400000 [00:13<00:00, 28931.20it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRj2iXAeyo1P",
        "outputId": "9d6f3313-7017-46e0-9d5c-d1af92e5e3d8"
      },
      "source": [
        "print('Size of input vocab : ', len(Review.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Review .vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17214\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 20111), (',', 17797), ('the', 15203), ('of', 11208), ('and', 11188), ('a', 11018), ('to', 7486), ('-', 6855), ('is', 6341), (\"'s\", 6323)]\n",
            "Labels :  defaultdict(None, {3: 0, 1: 1, 4: 2, 2: 3, 0: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfXt-Do-8qaL",
        "outputId": "63665210-7e14-4f4e-fb78-96cc28339f77"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRCDqmYz9l41"
      },
      "source": [
        "batch_size=64\n",
        "train_iterator , valid_iterator = data.BucketIterator.splits((train,valid),batch_size=batch_size,sort_key= lambda x: len(x.reviews),\n",
        "                                                             sort_within_batch=True,device=device)\n",
        "train_iterator , test_iterator = data.BucketIterator.splits((train,test),batch_size=batch_size,sort_key= lambda x: len(x.reviews),\n",
        "                                                             sort_within_batch=True,device=device)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8JLESZ-nBJ"
      },
      "source": [
        "import os, pickle\n",
        "with open('drive/My Drive/END2/Session5-Assignment/Model_artifacts/tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Review.vocab.stoi, tokens)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay3uIcSfbQdx"
      },
      "source": [
        "### Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZL-D6p9-5k6"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx, bidirectional=False):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        self.bidirectional = bidirectional\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=True)\n",
        "\n",
        "        \n",
        "        # Dense layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        scaling_factor = 2 if bidirectional else 1\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*scaling_factor, output_dim)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "      \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        if self.bidirectional:\n",
        "          hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "          hidden = self.dropout(hidden)\n",
        "          hidden = hidden[0]\n",
        "          \n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs, dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx1kFT2WxJ06"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Review.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 256\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "bidirectional=True\n",
        "PAD_IDX = Review.vocab.stoi[Review.pad_token]\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout, pad_idx=PAD_IDX, bidirectional=bidirectional)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsm2qC3uxpse"
      },
      "source": [
        "UNK_IDX = Review.vocab.stoi[Review.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sHEJlUEyAUA",
        "outputId": "876d963d-3d40-4254-9ad3-5b259d5c7228"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(17214, 300, padding_idx=1)\n",
            "  (encoder): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n",
            "The model has 7,886,509 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRtCn53syB_b"
      },
      "source": [
        "#### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkV2eyLAyFgP"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "# lr=0.005\n",
        "# lr = 2e-4\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT2Y9tGIyZyr"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUhCKEVbyYUO"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        review, review_lengths = batch.reviews   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(review, review_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        #acc = binary_accuracy(predictions, batch.labels)   \n",
        "        acc = categorical_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9K4wXEkyl8T"
      },
      "source": [
        "Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdgIVTeKyrsf"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            review, review_lengths = batch.reviews \n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(review, review_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            # acc = binary_accuracy(predictions, batch.labels)\n",
        "            acc = categorical_accuracy(predictions, batch.labels)   \n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFQX6r3ny6SX"
      },
      "source": [
        "Model Training and Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNnP87_Zy1Z3",
        "outputId": "f80680e2-9030-444a-d6ae-6ecb473e9365"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'drive/My Drive/END2/Session5-Assignment/Model_artifacts/saved_weights.pt')\n",
        "    \n",
        "    print(f'\\t Epoch: {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Epoch: {epoch} | Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Epoch: 0 | Train Loss: 1.559 | Train Acc: 31.07%\n",
            "\t Epoch: 0 | Val. Loss: 1.510 |  Val. Acc: 37.03% \n",
            "\n",
            "\t Epoch: 1 | Train Loss: 1.505 | Train Acc: 37.86%\n",
            "\t Epoch: 1 | Val. Loss: 1.505 |  Val. Acc: 38.24% \n",
            "\n",
            "\t Epoch: 2 | Train Loss: 1.451 | Train Acc: 44.02%\n",
            "\t Epoch: 2 | Val. Loss: 1.498 |  Val. Acc: 37.81% \n",
            "\n",
            "\t Epoch: 3 | Train Loss: 1.415 | Train Acc: 47.74%\n",
            "\t Epoch: 3 | Val. Loss: 1.496 |  Val. Acc: 39.38% \n",
            "\n",
            "\t Epoch: 4 | Train Loss: 1.378 | Train Acc: 51.71%\n",
            "\t Epoch: 4 | Val. Loss: 1.482 |  Val. Acc: 40.15% \n",
            "\n",
            "\t Epoch: 5 | Train Loss: 1.346 | Train Acc: 55.10%\n",
            "\t Epoch: 5 | Val. Loss: 1.489 |  Val. Acc: 40.33% \n",
            "\n",
            "\t Epoch: 6 | Train Loss: 1.333 | Train Acc: 56.37%\n",
            "\t Epoch: 6 | Val. Loss: 1.497 |  Val. Acc: 38.94% \n",
            "\n",
            "\t Epoch: 7 | Train Loss: 1.312 | Train Acc: 58.66%\n",
            "\t Epoch: 7 | Val. Loss: 1.496 |  Val. Acc: 39.64% \n",
            "\n",
            "\t Epoch: 8 | Train Loss: 1.291 | Train Acc: 60.69%\n",
            "\t Epoch: 8 | Val. Loss: 1.497 |  Val. Acc: 40.32% \n",
            "\n",
            "\t Epoch: 9 | Train Loss: 1.273 | Train Acc: 62.59%\n",
            "\t Epoch: 9 | Val. Loss: 1.489 |  Val. Acc: 40.07% \n",
            "\n",
            "\t Epoch: 10 | Train Loss: 1.256 | Train Acc: 64.28%\n",
            "\t Epoch: 10 | Val. Loss: 1.501 |  Val. Acc: 39.56% \n",
            "\n",
            "\t Epoch: 11 | Train Loss: 1.244 | Train Acc: 65.61%\n",
            "\t Epoch: 11 | Val. Loss: 1.502 |  Val. Acc: 39.64% \n",
            "\n",
            "\t Epoch: 12 | Train Loss: 1.230 | Train Acc: 67.06%\n",
            "\t Epoch: 12 | Val. Loss: 1.504 |  Val. Acc: 38.60% \n",
            "\n",
            "\t Epoch: 13 | Train Loss: 1.219 | Train Acc: 68.01%\n",
            "\t Epoch: 13 | Val. Loss: 1.497 |  Val. Acc: 39.46% \n",
            "\n",
            "\t Epoch: 14 | Train Loss: 1.209 | Train Acc: 69.20%\n",
            "\t Epoch: 14 | Val. Loss: 1.489 |  Val. Acc: 41.20% \n",
            "\n",
            "\t Epoch: 15 | Train Loss: 1.199 | Train Acc: 70.07%\n",
            "\t Epoch: 15 | Val. Loss: 1.481 |  Val. Acc: 41.71% \n",
            "\n",
            "\t Epoch: 16 | Train Loss: 1.191 | Train Acc: 70.91%\n",
            "\t Epoch: 16 | Val. Loss: 1.480 |  Val. Acc: 41.97% \n",
            "\n",
            "\t Epoch: 17 | Train Loss: 1.180 | Train Acc: 72.04%\n",
            "\t Epoch: 17 | Val. Loss: 1.492 |  Val. Acc: 40.76% \n",
            "\n",
            "\t Epoch: 18 | Train Loss: 1.171 | Train Acc: 73.14%\n",
            "\t Epoch: 18 | Val. Loss: 1.499 |  Val. Acc: 40.07% \n",
            "\n",
            "\t Epoch: 19 | Train Loss: 1.166 | Train Acc: 73.51%\n",
            "\t Epoch: 19 | Val. Loss: 1.503 |  Val. Acc: 39.03% \n",
            "\n",
            "\t Epoch: 20 | Train Loss: 1.152 | Train Acc: 74.84%\n",
            "\t Epoch: 20 | Val. Loss: 1.503 |  Val. Acc: 39.64% \n",
            "\n",
            "\t Epoch: 21 | Train Loss: 1.152 | Train Acc: 75.00%\n",
            "\t Epoch: 21 | Val. Loss: 1.484 |  Val. Acc: 41.28% \n",
            "\n",
            "\t Epoch: 22 | Train Loss: 1.142 | Train Acc: 76.04%\n",
            "\t Epoch: 22 | Val. Loss: 1.488 |  Val. Acc: 41.02% \n",
            "\n",
            "\t Epoch: 23 | Train Loss: 1.138 | Train Acc: 76.38%\n",
            "\t Epoch: 23 | Val. Loss: 1.486 |  Val. Acc: 41.45% \n",
            "\n",
            "\t Epoch: 24 | Train Loss: 1.134 | Train Acc: 76.91%\n",
            "\t Epoch: 24 | Val. Loss: 1.488 |  Val. Acc: 41.37% \n",
            "\n",
            "\t Epoch: 25 | Train Loss: 1.125 | Train Acc: 77.66%\n",
            "\t Epoch: 25 | Val. Loss: 1.489 |  Val. Acc: 41.19% \n",
            "\n",
            "\t Epoch: 26 | Train Loss: 1.121 | Train Acc: 78.10%\n",
            "\t Epoch: 26 | Val. Loss: 1.479 |  Val. Acc: 42.41% \n",
            "\n",
            "\t Epoch: 27 | Train Loss: 1.115 | Train Acc: 78.74%\n",
            "\t Epoch: 27 | Val. Loss: 1.497 |  Val. Acc: 40.50% \n",
            "\n",
            "\t Epoch: 28 | Train Loss: 1.112 | Train Acc: 78.93%\n",
            "\t Epoch: 28 | Val. Loss: 1.498 |  Val. Acc: 39.98% \n",
            "\n",
            "\t Epoch: 29 | Train Loss: 1.108 | Train Acc: 79.48%\n",
            "\t Epoch: 29 | Val. Loss: 1.499 |  Val. Acc: 40.41% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474rOsh5z4Jw",
        "outputId": "5f2b17b0-4cff-4585-9dd5-aa7d87e44be8"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/My Drive/END2/Session5-Assignment/Model_artifacts/saved_weights.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.493 | Test Acc: 40.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWHih34hD-e"
      },
      "source": [
        "### Validation of the model by passing the reviews and observing it's outcome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBDMnYTzz7UY"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path = 'drive/My Drive/END2/Session5-Assignment/Model_artifacts/saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('drive/My Drive/END2/Session5-Assignment/Model_artifacts/tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_review(review):\n",
        "    # labels: very negative, negative, neutral, positive, very positive\n",
        "    # defaultdict(None, {3: 0, 1: 1, 4: 2, 2: 3, 0: 4})\n",
        "    categories = {4: \"very negative\", 1:\"negative\", 3:\"neutral\", 0:'positive', 2: 'very positive'}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(review)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpS-ZydtgNH2"
      },
      "source": [
        "test_sample_reviews= test_df.sample(10).reset_index(drop=True)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tolZa4jqJA7",
        "outputId": "355160da-9514-4a54-d1b5-ee98ba9a3716"
      },
      "source": [
        "actual_lable= {0: \"very negative\", 1:\"negative\", 2:\"neutral\", 3:'positive', 4: 'very positive'}\n",
        "\n",
        "for i in range(test_sample_reviews.shape[0]):\n",
        "  print(f'Review : {test_sample_reviews.sentence[i]}')\n",
        "  print(f'Actual Sentiment : {actual_lable[test_sample_reviews.labels[i]]} | Predicted Sentiment : {classify_review(test_sample_reviews.sentence[i])} \\n')\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review : The film 's bathos often overwhelms what could have been a more multifaceted look at this interesting time and place .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : very negative \n",
            "\n",
            "Review : What happens to John Q ?\n",
            "Actual Sentiment : neutral | Predicted Sentiment : positive \n",
            "\n",
            "Review : The movie has lots of dancing and fabulous music .\n",
            "Actual Sentiment : very positive | Predicted Sentiment : positive \n",
            "\n",
            "Review : This is a throwaway , junk-food movie whose rap soundtrack was better tended to than the film itself .\n",
            "Actual Sentiment : very negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : You might not buy the ideas .\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : The movie addresses a hungry need for PG-rated , nonthreatening family movies , but it does n't go too much further .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : neutral \n",
            "\n",
            "Review : Director Dirk Shafer and co-writer Greg Hinton ride the dubious divide where gay porn reaches for serious drama .\n",
            "Actual Sentiment : negative | Predicted Sentiment : positive \n",
            "\n",
            "Review : It took 19 predecessors to get THIS ?\n",
            "Actual Sentiment : very negative | Predicted Sentiment : neutral \n",
            "\n",
            "Review : An unsatisfying hybrid of Blair Witch and typical stalk-and-slash fare , where the most conservative protagonist is always the last one living .\n",
            "Actual Sentiment : negative | Predicted Sentiment : positive \n",
            "\n",
            "Review : It will probably prove interesting to Ram Dass fans , but to others it may feel like a parody of the mellow , peace-and-love side of the '60s counterculture .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : positive \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}