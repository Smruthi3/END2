{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session7-Assigment-sentimentanalyis_session5_without_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9n05nnYyN2njN/H7c4sLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Smruthi3/END2/blob/main/Session7-Assignment/Assignment-Part1/Session7_Assigment_sentimentanalyis_session5_without_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAT1uIXimolE"
      },
      "source": [
        "## Importing the standford data files"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEXy5osOm36W",
        "outputId": "9379b37f-e067-4124-c144-e06818e46ea7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phimi8sJsDJC"
      },
      "source": [
        "### Data overview\n",
        "Based on Readme file given by StandordSentiment Treebank, the following way sentence and sentiment label are mapped\n",
        "\n",
        "1. datasetSentences.txt contains sentence and corresponding index\n",
        "2. sentiment_labels contains phrase id and sentiment values ranging from 0 to 1. Here notice that there is no one to mapping between datasetSentences and sentiment_labels\n",
        "5. dictionary.txt contains all phrases and their IDs\n",
        "6. Now, perfom  left join on datasetSentences and dictionary using sentence and phrases\n",
        "7. As we have phrase id, again perform left join on previous dataset (obtained from step #6) and sentiment_labels using phrase_id and phrase id\n",
        "8. Here we get a final table with sentiment values\n",
        "9. Divide the sentiment values into 5 buckets [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0] and assign a label from 0 to 4\n",
        "10. Finally, split dataset into 70% train, 30% test \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VAtZkPxnB19"
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "DATA_DIR = 'drive/My Drive/END2/Session5-Assignment/stanfordSentimentTreebank/stanfordSentimentTreebank'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ryw7CwEmndxs",
        "outputId": "53ff2ff6-a57d-48fa-aaed-0752a54c1973"
      },
      "source": [
        "datasetSentences = pd.read_csv(os.path.join(DATA_DIR, 'datasetSentences.txt'), sep='\\t')\n",
        "datasetSentences.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index                                           sentence\n",
              "0               1  The Rock is destined to be the 21st Century 's...\n",
              "1               2  The gorgeously elaborate continuation of `` Th...\n",
              "2               3                     Effective but too-tepid biopic\n",
              "3               4  If you sometimes like to go to the movies to h...\n",
              "4               5  Emerges as something rare , an issue movie tha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoOnR-Y_N02f",
        "outputId": "acf2b1cb-3cec-46cf-c2ed-63feab69d56b"
      },
      "source": [
        "datasetSentences.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZLTAEuzp_ba",
        "outputId": "a7409660-303c-4bf1-9bc1-83753d3bc32e"
      },
      "source": [
        "sentiment_label = pd.read_csv(os.path.join(DATA_DIR, \"sentiment_labels.txt\"),sep='|')\n",
        "sentiment_label.head(),sentiment_label.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   phrase ids  sentiment values\n",
              " 0           0           0.50000\n",
              " 1           1           0.50000\n",
              " 2           2           0.44444\n",
              " 3           3           0.50000\n",
              " 4           4           0.42708, (239232, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjGlmoJNqJ2F",
        "outputId": "a4b0b908-57f4-4c1a-ad7f-879580854070"
      },
      "source": [
        "dictionary = pd.read_csv(os.path.join(DATA_DIR, \"dictionary.txt\"), sep='|',header=None)\n",
        "dictionary.columns = ['phrases', 'phrase_id']\n",
        "dictionary.head(),dictionary.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(       phrases  phrase_id\n",
              " 0            !          0\n",
              " 1          ! '      22935\n",
              " 2         ! ''      18235\n",
              " 3       ! Alas     179257\n",
              " 4  ! Brilliant      22936, (239232, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpZ66-6xqTmB",
        "outputId": "2c92609e-972e-42d7-99d2-d3333c3c61e8"
      },
      "source": [
        "dataset_with_phrase_id = datasetSentences.merge(dictionary, left_on='sentence', right_on ='phrases', how='left')\n",
        "dataset_with_phrase_id.head(), dataset_with_phrase_id.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   sentence_index  ... phrase_id\n",
              " 0               1  ...  226166.0\n",
              " 1               2  ...  226300.0\n",
              " 2               3  ...   13995.0\n",
              " 3               4  ...   14123.0\n",
              " 4               5  ...   13999.0\n",
              " \n",
              " [5 rows x 4 columns], (11855, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORibhsIkrOtd"
      },
      "source": [
        "ads = dataset_with_phrase_id.merge(sentiment_label, left_on='phrase_id', right_on='phrase ids', how='left')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c5247BqrXyb"
      },
      "source": [
        "ads['labels'] = [(0 if 0 <=i <=0.2 else (1 if 0.2<i<=0.4 else (2 if 0.4 <i<=0.6 else (3 if 0.6 <i<=0.8 else 4)))) for i in ads['sentiment values'] ]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MyCarf0Mwp2v",
        "outputId": "4424659a-a41f-4c3a-aedd-3284a0ef1f21"
      },
      "source": [
        "ads.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrases</th>\n",
              "      <th>phrase_id</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>226166.0</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>226300.0</td>\n",
              "      <td>0.83333</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>13995.0</td>\n",
              "      <td>0.51389</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>14123.0</td>\n",
              "      <td>0.73611</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>13999.0</td>\n",
              "      <td>0.86111</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ... labels\n",
              "0               1  ...      3\n",
              "1               2  ...      4\n",
              "2               3  ...      2\n",
              "3               4  ...      3\n",
              "4               5  ...      4\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Y6N6ljhmXC",
        "outputId": "0345bf2e-1020-4092-84f4-16a27b9a6926"
      },
      "source": [
        "ads.shape,ads.labels.value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11855, 7), 1    2971\n",
              " 3    2966\n",
              " 4    2342\n",
              " 2    2144\n",
              " 0    1432\n",
              " Name: labels, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3Tacm9o0HeP",
        "outputId": "0342e37b-a983-4139-8949-1cdd469f145e"
      },
      "source": [
        "ads.labels.value_counts()/sum(ads.labels.value_counts())*100"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    25.061156\n",
              "3    25.018979\n",
              "4    19.755377\n",
              "2    18.085196\n",
              "0    12.079291\n",
              "Name: labels, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh89YImhEpfX"
      },
      "source": [
        "### Defining Data Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o56V5L6PCcEv",
        "outputId": "fdb01e58-feff-4981-e647-bd443eed6a89"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f515e43d950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2HpASKSE1NM"
      },
      "source": [
        "Review = data.Field(sequential = True , tokenize ='spacy',batch_first = True , include_lengths =True)\n",
        "Label = data.LabelField(tokenize ='spacy',is_target=True,batch_first=True,sequential=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIKcC6qeFZfp"
      },
      "source": [
        "fields = [('reviews', Review),('labels',Label)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5RBHBZxMO2J"
      },
      "source": [
        "review_examples = [data.Example.fromlist([ads.sentence[i],ads.labels[i]], fields) for i in range(ads.shape[0])] \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNd5rOkTLJlv"
      },
      "source": [
        "review_dataset = data.Dataset(review_examples,fields)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd8omXE03RI"
      },
      "source": [
        "(train, test) = review_dataset.split(split_ratio=[0.70, 0.30], random_state=random.seed(SEED))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHYqtEq7SrhN",
        "outputId": "0a4a559e-9748-4232-d96a-cfc1e24375d5"
      },
      "source": [
        "(len(train), len(test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8298, 3557)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6yAtlMTSwUe",
        "outputId": "4b2eabe1-98ab-44e2-ac61-677dd6ea3f43"
      },
      "source": [
        "vars(train.examples[10])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 0,\n",
              " 'reviews': ['In',\n",
              "  'other',\n",
              "  'words',\n",
              "  ',',\n",
              "  'about',\n",
              "  'as',\n",
              "  'bad',\n",
              "  'a',\n",
              "  'film',\n",
              "  'you',\n",
              "  \"'re\",\n",
              "  'likely',\n",
              "  'to',\n",
              "  'see',\n",
              "  'all',\n",
              "  'year',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2eBgtbwwNGm"
      },
      "source": [
        "#### Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAu3rEArSygI",
        "outputId": "4e8909b8-403e-4e9c-b063-98f0cac8d6bc"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "Review.build_vocab(train, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.38MB/s]                           \n",
            " 99%|█████████▉| 397673/400000 [00:13<00:00, 29481.43it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRj2iXAeyo1P",
        "outputId": "1779f72d-cce5-48d0-ac1c-9b69a02cf266"
      },
      "source": [
        "print('Size of input vocab : ', len(Review.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Review .vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17071\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 7833), (',', 7004), ('the', 5925), ('and', 4349), ('of', 4288), ('a', 4264), ('to', 2972), ('-', 2645), (\"'s\", 2515), ('is', 2510)]\n",
            "Labels :  defaultdict(None, {3: 0, 1: 1, 4: 2, 2: 3, 0: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfXt-Do-8qaL",
        "outputId": "d7c5c4e5-6b92-4e19-d6de-1c4d7dece63a"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRCDqmYz9l41"
      },
      "source": [
        "batch_size=64\n",
        "train_iterator , test_iterator = data.BucketIterator.splits((train,test),batch_size=batch_size,sort_key= lambda x: len(x.reviews),\n",
        "                                                             sort_within_batch=True,device=device)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8JLESZ-nBJ"
      },
      "source": [
        "import os, pickle\n",
        "with open('drive/My Drive/END2/Session7-Assignment/Assignment-part1/tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Review.vocab.stoi, tokens)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay3uIcSfbQdx"
      },
      "source": [
        "### Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZL-D6p9-5k6"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx, bidirectional=False):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        self.bidirectional = bidirectional\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=True)\n",
        "\n",
        "        \n",
        "        # Dense layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        scaling_factor = 2 if bidirectional else 1\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*scaling_factor, output_dim)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "      \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        if self.bidirectional:\n",
        "          hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "          hidden = self.dropout(hidden)\n",
        "          hidden = hidden[0]\n",
        "          \n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs, dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx1kFT2WxJ06"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Review.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 256\n",
        "num_output_nodes = 5\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "bidirectional=True\n",
        "PAD_IDX = Review.vocab.stoi[Review.pad_token]\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout, pad_idx=PAD_IDX, bidirectional=bidirectional)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsm2qC3uxpse"
      },
      "source": [
        "UNK_IDX = Review.vocab.stoi[Review.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sHEJlUEyAUA",
        "outputId": "39b5784b-6810-407d-fc40-573135486aba"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(17071, 300, padding_idx=1)\n",
            "  (encoder): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n",
            "The model has 7,843,609 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRtCn53syB_b"
      },
      "source": [
        "#### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkV2eyLAyFgP"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "# lr=0.005\n",
        "# lr = 2e-4\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT2Y9tGIyZyr"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUhCKEVbyYUO"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        review, review_lengths = batch.reviews   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(review, review_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        #acc = binary_accuracy(predictions, batch.labels)   \n",
        "        acc = categorical_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9K4wXEkyl8T"
      },
      "source": [
        "Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdgIVTeKyrsf"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            review, review_lengths = batch.reviews \n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(review, review_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            # acc = binary_accuracy(predictions, batch.labels)\n",
        "            acc = categorical_accuracy(predictions, batch.labels)   \n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFQX6r3ny6SX"
      },
      "source": [
        "Model Training and Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNnP87_Zy1Z3",
        "outputId": "88325517-c5be-4973-f33c-c4d7773d5af8"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    \n",
        "    # save the best model\n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        torch.save(model.state_dict(), 'drive/My Drive/END2/Session7-Assignment/Assignment-part1/saved_weights.pt')\n",
        "    \n",
        "    print(f'\\t Epoch: {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Epoch: {epoch} | Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}% \\n')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Epoch: 0 | Train Loss: 1.575 | Train Acc: 27.84%\n",
            "\t Epoch: 0 | Val. Loss: 1.541 |  Val. Acc: 33.01% \n",
            "\n",
            "\t Epoch: 1 | Train Loss: 1.527 | Train Acc: 35.10%\n",
            "\t Epoch: 1 | Val. Loss: 1.518 |  Val. Acc: 36.37% \n",
            "\n",
            "\t Epoch: 2 | Train Loss: 1.492 | Train Acc: 39.29%\n",
            "\t Epoch: 2 | Val. Loss: 1.519 |  Val. Acc: 36.92% \n",
            "\n",
            "\t Epoch: 3 | Train Loss: 1.461 | Train Acc: 42.65%\n",
            "\t Epoch: 3 | Val. Loss: 1.520 |  Val. Acc: 36.49% \n",
            "\n",
            "\t Epoch: 4 | Train Loss: 1.431 | Train Acc: 46.01%\n",
            "\t Epoch: 4 | Val. Loss: 1.518 |  Val. Acc: 36.69% \n",
            "\n",
            "\t Epoch: 5 | Train Loss: 1.401 | Train Acc: 49.09%\n",
            "\t Epoch: 5 | Val. Loss: 1.503 |  Val. Acc: 38.19% \n",
            "\n",
            "\t Epoch: 6 | Train Loss: 1.384 | Train Acc: 51.09%\n",
            "\t Epoch: 6 | Val. Loss: 1.505 |  Val. Acc: 38.39% \n",
            "\n",
            "\t Epoch: 7 | Train Loss: 1.362 | Train Acc: 53.57%\n",
            "\t Epoch: 7 | Val. Loss: 1.497 |  Val. Acc: 39.22% \n",
            "\n",
            "\t Epoch: 8 | Train Loss: 1.349 | Train Acc: 54.82%\n",
            "\t Epoch: 8 | Val. Loss: 1.502 |  Val. Acc: 38.58% \n",
            "\n",
            "\t Epoch: 9 | Train Loss: 1.321 | Train Acc: 57.74%\n",
            "\t Epoch: 9 | Val. Loss: 1.501 |  Val. Acc: 39.09% \n",
            "\n",
            "\t Epoch: 10 | Train Loss: 1.306 | Train Acc: 59.35%\n",
            "\t Epoch: 10 | Val. Loss: 1.494 |  Val. Acc: 40.04% \n",
            "\n",
            "\t Epoch: 11 | Train Loss: 1.299 | Train Acc: 59.87%\n",
            "\t Epoch: 11 | Val. Loss: 1.506 |  Val. Acc: 38.62% \n",
            "\n",
            "\t Epoch: 12 | Train Loss: 1.278 | Train Acc: 62.26%\n",
            "\t Epoch: 12 | Val. Loss: 1.504 |  Val. Acc: 38.68% \n",
            "\n",
            "\t Epoch: 13 | Train Loss: 1.264 | Train Acc: 63.74%\n",
            "\t Epoch: 13 | Val. Loss: 1.493 |  Val. Acc: 40.19% \n",
            "\n",
            "\t Epoch: 14 | Train Loss: 1.256 | Train Acc: 64.41%\n",
            "\t Epoch: 14 | Val. Loss: 1.493 |  Val. Acc: 40.11% \n",
            "\n",
            "\t Epoch: 15 | Train Loss: 1.237 | Train Acc: 66.58%\n",
            "\t Epoch: 15 | Val. Loss: 1.496 |  Val. Acc: 39.95% \n",
            "\n",
            "\t Epoch: 16 | Train Loss: 1.230 | Train Acc: 66.93%\n",
            "\t Epoch: 16 | Val. Loss: 1.486 |  Val. Acc: 41.16% \n",
            "\n",
            "\t Epoch: 17 | Train Loss: 1.228 | Train Acc: 67.39%\n",
            "\t Epoch: 17 | Val. Loss: 1.497 |  Val. Acc: 39.91% \n",
            "\n",
            "\t Epoch: 18 | Train Loss: 1.219 | Train Acc: 68.25%\n",
            "\t Epoch: 18 | Val. Loss: 1.491 |  Val. Acc: 40.30% \n",
            "\n",
            "\t Epoch: 19 | Train Loss: 1.218 | Train Acc: 68.19%\n",
            "\t Epoch: 19 | Val. Loss: 1.495 |  Val. Acc: 39.72% \n",
            "\n",
            "\t Epoch: 20 | Train Loss: 1.198 | Train Acc: 70.36%\n",
            "\t Epoch: 20 | Val. Loss: 1.499 |  Val. Acc: 39.44% \n",
            "\n",
            "\t Epoch: 21 | Train Loss: 1.191 | Train Acc: 71.13%\n",
            "\t Epoch: 21 | Val. Loss: 1.499 |  Val. Acc: 39.78% \n",
            "\n",
            "\t Epoch: 22 | Train Loss: 1.185 | Train Acc: 71.77%\n",
            "\t Epoch: 22 | Val. Loss: 1.497 |  Val. Acc: 40.23% \n",
            "\n",
            "\t Epoch: 23 | Train Loss: 1.176 | Train Acc: 72.59%\n",
            "\t Epoch: 23 | Val. Loss: 1.492 |  Val. Acc: 40.24% \n",
            "\n",
            "\t Epoch: 24 | Train Loss: 1.174 | Train Acc: 72.86%\n",
            "\t Epoch: 24 | Val. Loss: 1.491 |  Val. Acc: 40.59% \n",
            "\n",
            "\t Epoch: 25 | Train Loss: 1.166 | Train Acc: 73.51%\n",
            "\t Epoch: 25 | Val. Loss: 1.494 |  Val. Acc: 40.25% \n",
            "\n",
            "\t Epoch: 26 | Train Loss: 1.159 | Train Acc: 74.14%\n",
            "\t Epoch: 26 | Val. Loss: 1.490 |  Val. Acc: 40.54% \n",
            "\n",
            "\t Epoch: 27 | Train Loss: 1.160 | Train Acc: 74.12%\n",
            "\t Epoch: 27 | Val. Loss: 1.485 |  Val. Acc: 40.83% \n",
            "\n",
            "\t Epoch: 28 | Train Loss: 1.151 | Train Acc: 75.18%\n",
            "\t Epoch: 28 | Val. Loss: 1.488 |  Val. Acc: 40.81% \n",
            "\n",
            "\t Epoch: 29 | Train Loss: 1.142 | Train Acc: 76.07%\n",
            "\t Epoch: 29 | Val. Loss: 1.495 |  Val. Acc: 40.00% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWHih34hD-e"
      },
      "source": [
        "### Validation of the model by passing the reviews and observing it's outcome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBDMnYTzz7UY"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path = 'drive/My Drive/END2/Session7-Assignment/Assignment-part1/saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('drive/My Drive/END2/Session7-Assignment/Assignment-part1/tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# labels: very negative, negative, neutral, positive, very positive\n",
        "# defaultdict(None, {3: 0, 1: 1, 4: 2, 2: 3, 0: 4})\n",
        "categories = {4: \"very negative\", 1:\"negative\", 3:\"neutral\", 0:'positive', 2: 'very positive'}\n",
        "\n",
        "def classify_review(review):\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(review)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqWTcuiR9aqI"
      },
      "source": [
        "\n",
        "correctly_classified_review = []\n",
        "correctly_classified_act_label = []\n",
        "correctly_classified_pred_label = []\n",
        "\n",
        "miss_classified_review = []\n",
        "miss_classified_act_label = []\n",
        "miss_classified_pred_label = []\n",
        "\n",
        "for n in range(0,len(test)):\n",
        " # n = random.randint(0,len(test))\n",
        "  review = \" \".join(vars(test.examples[n])['reviews']).replace(',',\"\")\n",
        "  #print(f'Review : {review}')\n",
        "  act_label = categories[vars(test.examples[n])['labels']]\n",
        "  pred_label = classify_review(review)\n",
        "\n",
        "  if(act_label==pred_label):\n",
        "    correctly_classified_review.append(review)\n",
        "    correctly_classified_act_label.append(act_label)\n",
        "    correctly_classified_pred_label.append(pred_label)\n",
        "  else:\n",
        "    miss_classified_review.append(review)\n",
        "    miss_classified_act_label.append(act_label)\n",
        "    miss_classified_pred_label.append(pred_label)\n",
        "\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnDmCWcdBeTa"
      },
      "source": [
        "## Correctly classified reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0UWkTaMBjSg",
        "outputId": "1c1363c5-5aa9-48ce-a238-77addf5f9085"
      },
      "source": [
        "import random\n",
        "for i in range(0,10):\n",
        "  n=random.randint(0,len(correctly_classified_review))\n",
        "  print(f'Review : {correctly_classified_review[n]}')\n",
        "  print(f'Actual Sentiment : {correctly_classified_act_label[n]} | Predicted Sentiment : {correctly_classified_pred_label[n]} \\n')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review : Aloof and lacks any real raw emotion  which is fatal for a film that relies on personal relationships .\n",
            "Actual Sentiment : positive | Predicted Sentiment : positive \n",
            "\n",
            "Review : The explosion essentially ruined -- or  rather  overpowered -- the fiction of the movie for me .\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : The movie is like Scorsese 's Mean Streets redone by someone who ignored it in favor of old ` juvenile delinquent ' paperbacks with titles like Leather Warriors and Switchblade Sexpot .\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : Mention ` ` Solaris '' five years from now and I ' m sure those who saw it will have an opinion to share .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : neutral \n",
            "\n",
            "Review : A sleek advert for youthful anomie that never quite equals the sum of its pretensions .\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : Unexpected  and often contradictory  truths emerge .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : neutral \n",
            "\n",
            "Review : It 's really yet another anemic and formulaic Lethal Weapon - derived buddy - cop movie  trying to pass off its lack of imagination as hip knowingness .\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : The comedy Death to Smoochy is a rancorous curiosity : a movie without an apparent audience .\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : About as satisfying and predictable as the fare at your local drive through .\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : Why  you may ask  why should you buy the movie milk when the TV cow is free ?\n",
            "Actual Sentiment : negative | Predicted Sentiment : negative \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZhG2BmlC4LZ"
      },
      "source": [
        "## Miss classified reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4KrTQuXCGgQ",
        "outputId": "cb55a3cb-b6d7-4dcf-86d1-30dedb952090"
      },
      "source": [
        "import random\n",
        "for i in range(0,10):\n",
        "  n=random.randint(0,len(miss_classified_review))\n",
        "  print(f'Review : {miss_classified_review[n]}')\n",
        "  print(f'Actual Sentiment : {miss_classified_act_label[n]} | Predicted Sentiment : {miss_classified_pred_label[n]} \\n')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review : I ' ve never seen or heard anything quite like this film  and I recommend it for its originality alone .\n",
            "Actual Sentiment : very negative | Predicted Sentiment : negative \n",
            "\n",
            "Review : There 's really only one good idea in this movie  but the director runs with it and presents it with an unforgettable visual panache .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : positive \n",
            "\n",
            "Review : Your taste for Jonah - A Veggie Tales Movie may well depend on your threshold for pop manifestations of the Holy Spirit .\n",
            "Actual Sentiment : very positive | Predicted Sentiment : positive \n",
            "\n",
            "Review : Call me a cold - hearted curmudgeon for not being able to enjoy a mindless action movie  but I believe a movie can be mindless without being the peak of all things insipid .\n",
            "Actual Sentiment : positive | Predicted Sentiment : negative \n",
            "\n",
            "Review : One of the smarter offerings the horror genre has produced in recent memory  even if it 's far tamer than advertised .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : positive \n",
            "\n",
            "Review : The film delivers not just the full assault of Reno 's immense wit and insight  but a time travel back to what it felt like during those unforgettably uncertain days .\n",
            "Actual Sentiment : very negative | Predicted Sentiment : positive \n",
            "\n",
            "Review : Only in its final surprising shots does Rabbit - Proof Fence find the authority it 's looking for .\n",
            "Actual Sentiment : very positive | Predicted Sentiment : neutral \n",
            "\n",
            "Review : The fly - on - the - wall method used to document rural French school life is a refreshing departure from the now more prevalent technique of the docu - makers being a visible part of their work .\n",
            "Actual Sentiment : neutral | Predicted Sentiment : positive \n",
            "\n",
            "Review : A movie that harps on media - constructed ` issues ' like whether compromise is the death of self ... this Orgasm -LRB- wo n't be an -RRB- exceedingly memorable one for most people .\n",
            "Actual Sentiment : very negative | Predicted Sentiment : very positive \n",
            "\n",
            "Review : Please see previous answer .\n",
            "Actual Sentiment : very positive | Predicted Sentiment : negative \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}