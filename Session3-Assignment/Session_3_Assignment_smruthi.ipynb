{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 3-Assignment-smruthi.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Smruthi3/END2/blob/main/Session3-Assignment/Session_3_Assignment_smruthi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT9DFzYLIOGn"
      },
      "source": [
        "Write a neural network that can:\n",
        "* Take 2 inputs:\n",
        "  1. an image from MNIST dataset, and\n",
        "  2. a random number between 0 and 9\n",
        "* And gives two outputs:\n",
        "  1. the \"number\" that was represented by the MNIST image, and\n",
        "  2. the \"sum\" of this number with the random number that was generated and sent as the input to the network\n",
        "\n",
        "\n",
        "![image](https://drive.google.com/file/d/1tlNRJeYKzXiZW4jnhfHRYKal3l78cW4x/view?usp=sharing)\n",
        "\n",
        "* you can mix fully connected layers and convolution layers\n",
        "* you can use one-hot encoding to represent the random number input as well as the \"summed\" output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9EO7qjBxdxA"
      },
      "source": [
        "### Importing MNIST data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoiTWoREMJoB"
      },
      "source": [
        "## Importing required libraries\n",
        "import torch\n",
        "import torchvision # provide access to datasets, models, transforms, utils, etc\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_printoptions(linewidth=120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znjpWJGkMwp5"
      },
      "source": [
        "# Uploading MNIST dataset\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root='./data'\n",
        "    ,train=True\n",
        "    ,download=True\n",
        "    ,transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuOHva54MrBV"
      },
      "source": [
        "## Printing length of MNIST data set\n",
        "len(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgm2gcN4OnEg"
      },
      "source": [
        "## Observing distinct image lables\n",
        "train_set.targets.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8URbAplWvsLB"
      },
      "source": [
        "## Type of the dataset and number of samples from each image label\n",
        "print(type(train_set[0]))\n",
        "print(train_set.train_labels.bincount())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W2tdgSpPoJ4"
      },
      "source": [
        "### Sending data to dataloader to view the images in batch\n",
        "input_train_loader1 = torch.utils.data.DataLoader(train_set\n",
        "    ,batch_size=10\n",
        "    ,shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFiy8NujPzhh"
      },
      "source": [
        "batch = next(iter(input_train_loader1))\n",
        "images , labels = batch\n",
        "print(len(batch),type(batch))\n",
        "print(images.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpOcvGaFRR9P"
      },
      "source": [
        "grid = torchvision.utils.make_grid(images, nrow=10)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(np.transpose(grid, (1,2,0)))\n",
        "print('labels:', labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAONXcABd23x"
      },
      "source": [
        "### Data Gerenation and Represntation strategy\n",
        "Generating a Dataframe with two columns\n",
        "1. input = Numbers from 0 to 9\n",
        "2. output = Possible outcomes thats possible from summing up random numbers from 0 to 9 with MNIST data labels 0 to 9 \n",
        "3. Repeating this sample to 600 times to match the MNIST dataset length\n",
        "4. Custom data generation class is written to combine image and random numbder data by extending pytorch's Dataset class\n",
        "  * The random data set's input is converted into one hot encoding tensor\n",
        "  * This class returns the two inputs, image and random number (in one hot format) and two outputs, image label and summed output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RbUAUz8eXjN"
      },
      "source": [
        "## For loop to generate random number and summed output dataset\n",
        "import pandas as pd\n",
        "input = []\n",
        "output = []\n",
        "for i in range(0,10):\n",
        "  for j in range(0,10):\n",
        "    input.append(i)\n",
        "    output.append(i+j)\n",
        "    \n",
        "random_number_df=pd.DataFrame({'Input': input,\n",
        "              'Output': output}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0xhjrp2f3YX"
      },
      "source": [
        "random_number_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iuY70vaGZyd"
      },
      "source": [
        "random_number_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb6lNJOCstpw"
      },
      "source": [
        "### Repeating samples 600 times\n",
        "random_number_df=pd.concat([random_number_df]*600,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLyJN0QetC4m"
      },
      "source": [
        "random_number_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWia6OqkQkJu"
      },
      "source": [
        "### Customer dataset generator class \n",
        "from torch.utils.data import Dataset\n",
        "class RandomNumberdata(Dataset):\n",
        "  def __init__(self, img_df,input_df,):\n",
        "    self.img = img_df\n",
        "    self.data = input_df\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \n",
        "    images = self.img[index][0]\n",
        "    output1 = self.img[index][1]\n",
        "\n",
        "    r = self.data.iloc[index]\n",
        "    input_onehot = torch.nn.functional.one_hot(torch.tensor(self.data.Input,dtype=torch.long)) ## converting to one hot encoding\n",
        "    output = torch.tensor(self.data.Output,dtype=torch.long)\n",
        "    #output_onehot = torch.nn.functional.one_hot(torch.tensor(self.data.Output,dtype=torch.long))\n",
        "    output2 = output[r[0]]\n",
        "    sample = input_onehot[r[1]]\n",
        "    return images, output1, sample, output2 ## returning 4 objects as mentioned above\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN-nwZHiSYSA"
      },
      "source": [
        "## Calling an instance of custom dataset class\n",
        "combined_data_set = RandomNumberdata(train_set,random_number_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqTgBzyLThFP"
      },
      "source": [
        "print(len(combined_data_set))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJsaha4UvRnk"
      },
      "source": [
        "## View of one record from combined dataset \n",
        "combined_data_set[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1nt4Ufq0s-G"
      },
      "source": [
        "# combined_data_loader = torch.utils.data.DataLoader(combined_data_set, batch_size=10)\n",
        "\n",
        "# batch = next(iter(combined_data_loader))\n",
        "# images , output1, numbers,output2 = batch\n",
        "# print(len(batch),type(batch))\n",
        "# print(images.shape, output1.shape,numbers.shape, output2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Il5d5Rd5dZ"
      },
      "source": [
        "### Building neural network here\n",
        "The network consists of 2 conv2d layers, 4 linear layers \n",
        "\n",
        "1. The image is passed as a first input to network with consists of 2 conv2d layers followed by one linear layer with approriate activation function at each layer\n",
        "2. The random number passed as second input linear with layer with approriate activation function at each layer\n",
        "3. First and second inputs are combined using torch.cat function \n",
        "4. Concatenated input is sent to 3rd liner layer followed by softmax activation function to get first output\n",
        "5. The same  concatenated input is sent to 4th liner layer followed by softmax activation function to get second output\n",
        "6. Finally two outputs, image label and summed output are returned from the network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCmhyMnMpIn_"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=10, out_features=120)\n",
        "\n",
        "        self.out1 = nn.Linear(in_features=240, out_features=10)\n",
        "\n",
        "        self.out2 = nn.Linear(in_features=240, out_features=19)\n",
        "\n",
        "    def forward(self, img, rand): ## Passing two inputs here \n",
        "        \n",
        "        img = self.conv1(img)\n",
        "        img = F.relu(img)\n",
        "        img = F.max_pool2d(img, kernel_size=2, stride=2)\n",
        "\n",
        "     \n",
        "        img = self.conv2(img)\n",
        "        img= F.relu(img)\n",
        "        img= F.max_pool2d(img, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        img = img.reshape(-1, 12 * 4 * 4)\n",
        "        img = self.fc1(img)\n",
        "        img = F.relu(img)\n",
        "\n",
        "\n",
        "        rand = self.fc2(rand)\n",
        "        rand = F.relu(rand)\n",
        "\n",
        "        combined = torch.cat((img,rand),dim=1) ## combining two inputs here\n",
        "\n",
        "        #print(combined.shape)\n",
        "\n",
        "        out1 = self.out1(combined)\n",
        "        out1 = F.softmax(out1, dim=1) ## output1, image label\n",
        "\n",
        "\n",
        "        out2=self.out2(combined)\n",
        "        out2 = F.softmax(out1, dim=1) ## output2, summed output\n",
        "\n",
        "        return out1,out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3LfD-ISpcOw"
      },
      "source": [
        "## Enabling the GPU \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS6-qwLn9cWV"
      },
      "source": [
        "### Generating train and test loaders by  SubsetRandomSampler\n",
        "1. Data is split into 80% train and 20% test \n",
        "2. Model data is trained using training set and its perfomance is evaluated on test \n",
        "3. Cross entropy is used as loss function for both the output, because we are solving a classification problem and The purpose of the Cross-Entropy is to take the output probabilities (P) and measure the distance from the truth values\n",
        "4. Based on loss and accuracy model perfomance is evaluated "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAmGFq922dp4"
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "batch_size = 100\n",
        "test_split = .2\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and test splits:\n",
        "dataset_size = len(combined_data_set)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(test_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating  data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "testid_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(combined_data_set, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(combined_data_set, batch_size=batch_size,\n",
        "                                                sampler=testid_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa0-VCqE2F2B"
      },
      "source": [
        "len(train_loader),len(test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7hdTfOqRMjy"
      },
      "source": [
        "import torch.optim as optim\n",
        "network = Network()\n",
        "network = network.to(device) ## Adding network to gpu\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01) ## Defining an optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xde9n1WBboAp"
      },
      "source": [
        "## Defining accuracy function here\n",
        "def accuracy(preds, labels):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    correct = preds.argmax(dim=1).eq(labels).sum().item() #convert into float for division \n",
        "    acc = correct / len(labels)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDgxNVXRRMyW"
      },
      "source": [
        "## Defined the train function here which takes network, data iterator, optimizer and device\n",
        "## Trains the network batch wise\n",
        "def train(model, iterator, optimizer,device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc1 = 0\n",
        "    epoch_acc2 = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, output1, numbers,output2 = batch \n",
        "        images, output1, numbers,output2 = images.to(device), output1.to(device), numbers.to(device),output2.to(device) ## Adding all inputs to gpu\n",
        "\n",
        "       # print(images.shape)\n",
        "       # print(numbers.shape)\n",
        "\n",
        "        preds1,preds2 = model(images.float(),numbers.float()) # Pass Batch\n",
        "        \n",
        "        #preds1,preds2 = network(images.float().to(device),numbers.float().to(device))\n",
        "\n",
        "        loss1 = F.cross_entropy(preds1, output1)\n",
        "\n",
        "        loss2 = F.cross_entropy(preds2, output2)# Calculate Loss\n",
        "        \n",
        "        loss=loss1+loss2 ## Two losses are calculated seperately and combined later and sent to backpropagation\n",
        "     \n",
        "        acc1 = accuracy(preds1, output1)\n",
        "\n",
        "        acc2 = accuracy(preds2, output2)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # Calculate Gradients\n",
        "        optimizer.step() # Update Weights\n",
        "\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc1 += acc1\n",
        "        epoch_acc2 += acc2\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc1 / len(iterator), epoch_acc2 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r58Jc_E9TxGc"
      },
      "source": [
        "## Defined the evaluate function here which takes network, data iterator, optimizer and device\n",
        "## Trains the network batch wise\n",
        "\n",
        "def evaluate(model, iterator,device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc1 = 0\n",
        "    epoch_acc2 = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            images, output1, numbers,output2 = batch \n",
        "            images, output1, numbers,output2 = images.to(device), output1.to(device), numbers.to(device),output2.to(device)\n",
        "\n",
        "            preds1,preds2 = model(images.float(),numbers.float()) # Pass Batch\n",
        "        \n",
        "            loss1 = F.cross_entropy(preds1, output1)\n",
        "\n",
        "            loss2 = F.cross_entropy(preds2, output2)# Calculate Loss\n",
        "            \n",
        "            loss=loss1+loss2\n",
        "        \n",
        "            acc1 = accuracy(preds1, output1)\n",
        "\n",
        "            acc2 = accuracy(preds2, output2)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc1 += acc1\n",
        "            epoch_acc2 += acc2\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc1 / len(iterator), epoch_acc2 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnYEHcygVao2"
      },
      "source": [
        "## Function that calculates time taken per batch\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIuJ0YINV4Iu"
      },
      "source": [
        "## Running a network for 10 epochs\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc1, train_acc2 = train(network, train_loader, optimizer,device)\n",
        "    valid_loss, valid_acc1, valid_acc2 = evaluate(network, test_loader,device)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        #torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc of Image: {train_acc1*100:.2f} \\\n",
        "               | Train Acc of addition of image label and Random Number: {train_acc2*100:.2f} %')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc of Image: {valid_acc1*100:.2f} | \\\n",
        "              | Val. Acc of addition of image label and Random Number: {train_acc2*100:.2f} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g4zQy93B138"
      },
      "source": [
        "### Final Comments\n",
        "\n",
        "1. Combined loss on train and validation seems to be consistant from the 1st epoch and there is no much improvement seen\n",
        "2. The accuracy for image prediction looks good where as accuracy for summed output is very poor <10%\n",
        "3.Note that model is exectued only for 10 epochs as each epoch takes 15mins\n",
        "4.I attempted to solve the given problem based on my understanding. However feel there is an improvement needed in the solution approuch \n",
        "    * I understand cross entropy loss is used for classification problem and RMSE/MSE to be used while adding numbers or any regression problem\n",
        "    * I converted this entire problem as muti node classification, where first output node classifys a given image out of 10 labels and second output node  with 19 lables (each of them represents the sum of the image lable and random number ) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy9p59pwTP4u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}